{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤러(WebCrawler) 웹페이지의 데이터를 모으는 것\n",
    "#pip install requests(설치하고 난 뒤에 진행해야함)\n",
    "\n",
    "import requests #requests 모듈을 가져옴(PUT, GET, POST, DELETE)\n",
    "\n",
    "url = \"http://www.daum.net\" # requests.get 함수를 활용하기 전에 변수값을 정의해 줘야함\n",
    "response = requests.get(url) #해당 모듈을 사용할때는 모듈의 이름을 작성해야함\n",
    "\n",
    "#print(response.text) #응답된 response의 text값을 불러옴\n",
    "\n",
    "#print(response.url) #응답된 response의 url을 가져옴\n",
    "\n",
    "#print(response.content) #모든 컨텐츠를 가져옴\n",
    "\n",
    "#print(response.encoding) #인코딩된 방식을 가져옴 \n",
    "\n",
    "#print(response.headers)\n",
    "\n",
    "#print(response.json) # json의 응답값을 가져옴, 200이면 양호\n",
    "\n",
    "#print(response.links) # 해당 response의 링크데이터를 가져옴\n",
    "\n",
    "#print(response.ok)\n",
    "\n",
    "#print(response.status_code) # 200이면 양호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "import requests #requests 모듈을 가져옴(PUT, GET, POST, DELETE)\n",
    "from bs4 import BeautifulSoup #bs4 모듈의 BeautifulSoup 기능을 가져오는 것임\n",
    "\n",
    "url = \"http://www.daum.net\" # requests.get 함수를 활용하기 전에 변수값을 정의해 줘야함\n",
    "response = requests.get(url) #해당 모듈을 사용할때는 모듈의 이름을 작성해야함\n",
    "print(type(response.text)) #response의 text값을 가져옴\n",
    "#type 함수는 형태를 확인하는 함수\n",
    "print(type(BeautifulSoup(response.text, 'html.parser'))) # 데이터를 기반으로 의미있는 데이터로 변환하는 것을 의미함\n",
    "#str 데이터를 bs4.BeautifulSoup라는 객체 데이터로 변환함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup란?\n",
    "데이터, 파싱방법을 필요로 함\n",
    "bs4라는 모듈안에 있는 기능(함수)임\n",
    "데이터 종류는 html과 xml이 활용가능.()\n",
    "Parsing이란 데이터를 분석하여 의미있게 변경하는 것 / html.parser를 활용할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 위 :  코로나19 발생현황 \n",
      "\n",
      "2 위 :  이경규X이용진 \n",
      "\n",
      "3 위 :  리튬인산철배터리 \n",
      "\n",
      "4 위 :  신동미 출연확정 \n",
      "\n",
      "5 위 :  보이차파는곳 \n",
      "\n",
      "6 위 :  사회적 거리두기 \n",
      "\n",
      "7 위 :  개미는오늘도뚠뚠3 \n",
      "\n",
      "8 위 :  자동차다이렉트보험 \n",
      "\n",
      "9 위 :  온유 캐스팅 \n",
      "\n",
      "10 위 :  철판가격 \n",
      "\n",
      "11 위 :  김향기 계약만료 \n",
      "\n",
      "12 위 :  표예진 모범택시 \n",
      "\n",
      "13 위 :  32인치모니터 \n",
      "\n",
      "14 위 :  아이콘 1위 \n",
      "\n",
      "15 위 :  콩나물재배기 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests #requests 모듈을 가져옴(PUT, GET, POST, DELETE)\n",
    "from bs4 import BeautifulSoup #bs4 모듈의 BeautifulSoup 기능을 가져오는 것임\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"http://www.daum.net\" # requests.get 함수를 활용하기 전에 변수값을 정의해 줘야함\n",
    "response = requests.get(url) #해당 모듈을 사용할때는 모듈의 이름을 작성해야함\n",
    "#print(response.text[:500]) #response의 text값을 가져옴\n",
    "#type 함수는 형태를 확인하는 함수\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#parsing된 데이터를 soup라는 변수로 선언함\n",
    "rank = 1\n",
    "#순위를 체크하기 위해 랭크변수를 선언\n",
    "\n",
    "#file = open(\"daum.html\", \"w\", encoding =\"UTF8\") // file = open(파일명, 모드, 버퍼 설정, 인코딩 설정)\n",
    "# r = read(읽기) / w = write(쓰기) / a = append(덮어쓰기)\n",
    "#file.write(response.text)\n",
    "#file.close()\n",
    "\n",
    "# a태크 내에 class 값이 내림차순으로 되어있음\n",
    "            \n",
    "#print(soup.title)\n",
    "#print(soup.title.string) #원하는 데이터를 추출하는 방법\n",
    "#print(soup.span)\n",
    "#print(soup.findAll('span')) \n",
    "\n",
    "results = soup.findAll('a', 'link_favorsch') # 해당 html에서 a태그와 link_favorsch 값이 포함된 값을 가져옴\n",
    "\n",
    "search_rank_file = open(\"rankresult.txt\", \"a\") # rankresult.txt 생성\n",
    "\n",
    "#print(datetime.today().strftime(\"%Y년 %m월 %d일의 실시간 검색어 순위입니다.\\n\"))\n",
    "\n",
    "for result in results : #result를 results까지 모두 확인함\n",
    "    search_rank_file.write(str(rank)+\"위:\"+result.get_text()+\"\\n\")\n",
    "    print(rank,\"위 : \",result.get_text(), \"\\n\") # result.get_text()로 텍스트만 가져옴. \\n은 줄바꿈\n",
    "    rank += 1 #등호를 기준으로 좌측에 있는 변수에, 우측에 있는 덧셈 수식을 넣는 것을 줄여서 += 연산자를 통해 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 위 :  함소원 진화 \n",
      "\n",
      "2 위 :  함소원 \n",
      "\n",
      "3 위 :  찬열 \n",
      "\n",
      "4 위 :  제니 지디 \n",
      "\n",
      "5 위 :  기성용 \n",
      "\n",
      "6 위 :  오은영 \n",
      "\n",
      "7 위 :  제니 \n",
      "\n",
      "8 위 :  진화 \n",
      "\n",
      "9 위 :  이주연 \n",
      "\n",
      "10 위 :  지디 \n",
      "\n",
      "11 위 :  지드래곤 \n",
      "\n",
      "12 위 :  임영웅 대학교 \n",
      "\n",
      "13 위 :  서현민 \n",
      "\n",
      "14 위 :  김현우 \n",
      "\n",
      "15 위 :  박찬열 \n",
      "\n",
      "16 위 :  변정하 \n",
      "\n",
      "17 위 :  이지혜 \n",
      "\n",
      "18 위 :  범죄도시 \n",
      "\n",
      "19 위 :  파오차이 \n",
      "\n",
      "20 위 :  프렌즈 김현우 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests #requests 모듈을 가져옴(PUT, GET, POST, DELETE)\n",
    "from bs4 import BeautifulSoup #bs4 모듈의 BeautifulSoup 기능을 가져오는 것임\n",
    "from datetime import datetime\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "#네이버와 같은 보안모듈이 작동하고 있는 페이지는 헤더스를 추가해서 요청해야함\n",
    "url = \"https://datalab.naver.com/keyword/realtimeList.naver?age=20s\" # requests.get 함수를 활용하기 전에 변수값을 정의해 줘야함\n",
    "response = requests.get(url, headers=headers) #해당 모듈을 사용할때는 모듈의 이름을 작성해야함 // 헤더스에 대한 내용을 작성해주어야함\n",
    "#print(response.text[:500]) #response의 text값을 가져옴\n",
    "#type 함수는 형태를 확인하는 함수\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#parsing된 데이터를 soup라는 변수로 선언함\n",
    "rank = 1\n",
    "#순위를 체크하기 위해 랭크변수를 선언\n",
    "\n",
    "#file = open(\"daum.html\", \"w\", encoding =\"UTF8\") // file = open(파일명, 모드, 버퍼 설정, 인코딩 설정)\n",
    "# r = read(읽기) / w = write(쓰기) / a = append(덮어쓰기)\n",
    "#file.write(response.text)\n",
    "#file.close()\n",
    "\n",
    "# a태크 내에 class 값이 내림차순으로 되어있음\n",
    "            \n",
    "#print(soup.title)\n",
    "#print(soup.title.string) #원하는 데이터를 추출하는 방법\n",
    "#print(soup.span)\n",
    "#print(soup.findAll('span')) \n",
    "\n",
    "#네이버는 span - item_title을 갖고 있음\n",
    "results = soup.findAll('span', 'item_title') # 해당 html에서 a태그와 link_favorsch 값이 포함된 값을 가져옴\n",
    "\n",
    "search_rank_file = open(\"rankresult_naver.txt\", \"a\") # rankresult.txt 생성\n",
    "\n",
    "#print(datetime.today().strftime(\"%Y년 %m월 %d일의 실시간 검색어 순위입니다.\\n\"))\n",
    "\n",
    "for result in results : #result를 results까지 모두 확인함\n",
    "    search_rank_file.write(str(rank)+\"위:\"+result.get_text()+\"\\n\")\n",
    "    print(rank,\"위 : \",result.get_text(), \"\\n\") # result.get_text()로 텍스트만 가져옴. \\n은 줄바꿈\n",
    "    rank += 1 #등호를 기준으로 좌측에 있는 변수에, 우측에 있는 덧셈 수식을 넣는 것을 줄여서 += 연산자를 통해 사용가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
